{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['normalize']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "# Created in 2015-02-26 by Alberto Ueda\n",
    "# Generates the input to ca-index from the MAS, DBLP data.\n",
    "%pylab inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mmap\n",
    "import time\n",
    "import re\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "CS_AREA = 2        \n",
    "UTF8 = 'utf-8'        \n",
    "\n",
    "class Dict(dict):\n",
    "    def __missing__(self, key):\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: MAS - Venues and Subdomains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf     = pd.read_csv(\"../azure/Conference.csv\")\n",
    "conf_cat = pd.read_csv(\"../azure/Conf_Category.csv\")\n",
    "jour     = pd.read_csv(\"../azure/Journal.csv\")\n",
    "jour_cat = pd.read_csv(\"../azure/Jour_Category.csv\")\n",
    "cat      = pd.read_csv(\"../azure/Category.csv\")\n",
    "dom      = pd.read_csv(\"../azure/Domain.csv\")\n",
    "\n",
    "# All: Pre-Merge\n",
    "cat.rename(columns={'Name':'SubDomainName'}, inplace=True)\n",
    "dom.rename(columns={'Name':'DomainName'}, inplace=True)\n",
    "\n",
    "# Conferences: Pre-Merge\n",
    "conf.rename(columns={'ID':'VenueOldID'}, inplace=True)\n",
    "conf_cat.rename(columns={'ConfCategoryID':'CategoryID'}, inplace=True)\n",
    "\n",
    "# Conferences: Merges\n",
    "conf_and_dom = pd.merge(conf.ix[:, :3], conf_cat, \n",
    "                        left_on='VenueOldID', \n",
    "                        right_on='CategoryID', \n",
    "                        how='left')\n",
    "conf_and_subdom_names = pd.merge(conf_and_dom[conf_and_dom['DomainID'] == CS_AREA], cat, \n",
    "                                 left_on=['DomainID', 'SubDomainID'], \n",
    "                                 right_on=['DomainID', 'SubDomainID'], \n",
    "                                 how='left')\n",
    "conf_and_dom_names = pd.merge(conf_and_subdom_names, dom,\n",
    "                              left_on='DomainID', \n",
    "                              right_on='ID', \n",
    "                              suffixes=['SubDomain','Domain'],\n",
    "                              how='left')\n",
    "conf_and_dom_names['Type'] = 'C'\n",
    "\n",
    "# Journals: Pre-Merge\n",
    "jour.rename(columns={'ID':'VenueOldID'}, inplace=True)\n",
    "jour_cat.rename(columns={'CJourID':'CategoryID'}, inplace=True)\n",
    "\n",
    "# Journals: Merges\n",
    "jour_and_dom = pd.merge(jour.ix[:, :3], jour_cat, \n",
    "                        left_on='VenueOldID', \n",
    "                        right_on='CategoryID', \n",
    "                        how='left')\n",
    "jour_and_subdom_names = pd.merge(jour_and_dom[jour_and_dom['DomainID'] == CS_AREA], cat, \n",
    "                                 left_on=['DomainID', 'SubDomainID'], \n",
    "                                 right_on=['DomainID', 'SubDomainID'], \n",
    "                                 how='left')\n",
    "jour_and_dom_names = pd.merge(jour_and_subdom_names, dom,\n",
    "                              left_on='DomainID', \n",
    "                              right_on='ID', \n",
    "                              suffixes=['SubDomain','Domain'],\n",
    "                              how='left')\n",
    "jour_and_dom_names['Type'] = 'J'\n",
    "\n",
    "venues_subdomains = pd.concat([conf_and_dom_names, jour_and_dom_names], ignore_index=True)\n",
    "venues_subdomains.to_csv('../azure/mas-venue-subdomain.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: DBLP - CNPq Author's Publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Finished after 156.600 seconds. ---\n"
     ]
    }
   ],
   "source": [
    "# Time\n",
    "start_time = time.time()\n",
    "\n",
    "# CNPQ Authors\n",
    "cnpq_authors = pd.read_csv(\"../cnpq/prod-levels.csv\")\n",
    "cnpq_authors = cnpq_authors.ix[:, ['Name', 'ShortName', 'CNPqLevel']]\n",
    "fullnames_list = [unidecode(x.decode(UTF8)).upper() for x in cnpq_authors['Name'].tolist()]\n",
    "shortnames_list = [unidecode(x.decode(UTF8)).upper() for x in cnpq_authors['ShortName'].tolist()]\n",
    "\n",
    "# First + last names\n",
    "simplenames_list = []\n",
    "\n",
    "# Names Pre-processing\n",
    "for fullname in fullnames_list:\n",
    "    names = fullname.split()\n",
    "    simplename = names[0] + ' ' + names[-1]\n",
    "    simplenames_list.append(simplename)\n",
    "    \n",
    "# DBLP Authors\n",
    "with open('../dblp/dblp-sorted.pubsfile') as dblp_file, open('../dblp/cnpq-authors-dblpkeys.pubsfile', 'wb') as output:\n",
    "    selected_lines = []    \n",
    "    used_simplenames = Dict()\n",
    "    copy_pubs = False\n",
    "\n",
    "    for line in dblp_file:\n",
    "\n",
    "        if '\\t' in line:\n",
    "            if copy_pubs:\n",
    "                # Adding author's publications\n",
    "                selected_lines.append(line)\n",
    "            continue\n",
    "\n",
    "        # Reseting flags\n",
    "        copy_pubs = False\n",
    "        found = False\n",
    "        dblp_author = unidecode(line.strip('\\n').decode(UTF8)).upper()\n",
    "\n",
    "        # It is a name. Generating simplename for check.\n",
    "        names = dblp_author.split()\n",
    "        simplename = names[0] + ' ' + names[-1]\n",
    "\n",
    "        # Searching the new author             \n",
    "        # Heuristic: When there are more than one option for name,\n",
    "        #            choose the one with more publications        \n",
    "        if used_simplenames[simplename] == False and dblp_author in fullnames_list:\n",
    "            found = True\n",
    "        elif used_simplenames[simplename] == False and dblp_author in shortnames_list:\n",
    "            found = True\n",
    "            #line = line.strip('\\n') + ' (By ShortName)\\n'\n",
    "        elif used_simplenames[simplename] == False and dblp_author in simplenames_list:\n",
    "            found = True\n",
    "            #line = line.strip('\\n') + ' (By SimpleName)\\n'\n",
    "                \n",
    "        if not found: \n",
    "            continue\n",
    "        else:\n",
    "            # Adding author's name\n",
    "            selected_lines.append(line)\n",
    "            used_simplenames[simplename] = True\n",
    "            copy_pubs = True\n",
    "        \n",
    "    print(\"\\n--- Finished after %.3f seconds. ---\" % (time.time() - start_time))\n",
    "\n",
    "    for line in selected_lines:\n",
    "        output.write(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: DBLP - CNPq Author's Venues"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BASH: Execute from the directory of 'cnpq-authors-dblpkeys.pubsfile' (usually 'ca-index/data/dblp')\n",
    "# less cnpq-authors-dblpkeys.pubsfile | grep -Po '\\tconf/.*(?=/)|\\tjournals/.*(?=/)' | tr -d \\\\t | sort | uniq > cnpq-all-vkeys.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: DBLP - Merging DBLP Venues Info with CNPq Venues' Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vkeys = pd.read_csv('../dblp/cnpq-all-vkeys.txt', header=None)\n",
    "confs = pd.read_csv('../dblp/dblp-conf-azlist2.csv')\n",
    "journals = pd.read_csv('../dblp/dblp-journals-azlist2.csv')\n",
    "\n",
    "# Pre-processing columns names\n",
    "vkeys.columns = ['VKey']\n",
    "confs.rename(columns={'Unnamed: 0':'VKey'}, inplace=True)\n",
    "journals.rename(columns={'Unnamed: 0':'VKey'}, inplace=True)\n",
    "all_dblp_venues = pd.concat([confs, journals])\n",
    "\n",
    "# Merge\n",
    "all_cnpq_authors_venues = pd.merge(vkeys, all_dblp_venues, how='left')\n",
    "all_cnpq_authors_venues.to_csv('../dblp/cnpq-authors-vkeys.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: DBLP & MAS - Venues Conciliation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Type for:  cluster computing !!!\n",
      "Distinct Type for:  human-computer interaction !!!\n",
      "Distinct Type for:  computational intelligence !!!\n",
      "Distinct Type for:  intelligent data analysis !!!\n",
      "Distinct Type for:  requirements engineering !!!\n",
      "Distinct Type for:  operating systems review !!!\n",
      "Distinct Type for:  automated software engineering !!!\n",
      "Distinct Type for:  automated software engineering !!!\n",
      "Distinct Type for:  computational intelligence !!!\n",
      "Distinct Type for:  cluster computing !!!\n",
      "Distinct Type for:  intelligent data analysis !!!\n",
      "Distinct Type for:  operations research !!!\n",
      "Distinct Type for:  information retrieval !!!\n",
      "Distinct Type for:  language resources and evaluation !!!\n",
      "Distinct Type for:  multimedia systems !!!\n",
      "Distinct Type for:  parallel computing !!!\n",
      "Distinct Type for:  requirements engineering !!!\n",
      "Distinct Type for:  theoretical computer science !!!\n",
      "Distinct Type for:  vlsi design !!!\n",
      "Distinct Type for:  virtual reality !!!\n"
     ]
    }
   ],
   "source": [
    "dblp = pd.read_csv('../dblp/cnpq-authors-vkeys.csv')\n",
    "mas = pd.read_csv('../azure/mas-venue-subdomain.csv')\n",
    "\n",
    "# Pre-processing tables\n",
    "def normalize(x):\n",
    "    return unidecode(str(x).decode('utf-8)')).lower()\n",
    "\n",
    "dblp.rename(columns={'VKey':'DBLPVenueKey', 'NameH1':'DBLPName', 'ShortNameAZList':'DBLPShortName'}, inplace=True)\n",
    "dblp['DBLPShortName'] = dblp[dblp['DBLPShortName'].notnull()]['DBLPShortName'].apply(lambda x: normalize(x))\n",
    "dblp['DBLPName'] = dblp['DBLPName'].apply(lambda x: normalize(x))\n",
    "\n",
    "dblp['DBLPType'] = dblp['DBLPVenueKey'].apply(lambda x: 'C' if x.startswith('conf/') else 'J')\n",
    "dblp['DBLPMinVKey'] = dblp['DBLPVenueKey'].apply(lambda x: normalize(x).replace(\"conf/\", \"\").replace(\"journals/\", \"\"))\n",
    "\n",
    "mas.rename(columns={'VenueOldID':'MASID', 'FullName':'MASName', 'Type':'MASType', 'ShortName':'MASShortName', \n",
    "                    'SubDomainName':'MASSubDomainName'}, inplace=True)\n",
    "mas = mas.ix[:, ['MASID', 'MASType', 'MASShortName', 'MASName', 'DomainName', 'MASSubDomainName', 'DomainID', 'SubDomainID']]\n",
    "\n",
    "mas['MASShortName'] = mas[mas['MASShortName'].notnull()]['MASShortName'].apply(lambda x: normalize(x))\n",
    "mas['MASName'] = mas['MASName'].apply(lambda x: normalize(x))\n",
    "\n",
    "# For Tests\n",
    "# dblp = dblp.head(100)\n",
    "#[dblp['DBLPName'] == 'artificial intelligence in education']\n",
    "\n",
    "result = pd.DataFrame()\n",
    "dblp_found = Dict()\n",
    "\n",
    "for index, row in dblp.iterrows():\n",
    "    dblp_key = row['DBLPVenueKey']\n",
    "    \n",
    "    if dblp_found[dblp_key] == 'True': continue \n",
    "    \n",
    "    # First attempt: By Venue Fullname    \n",
    "    dblp_name = row['DBLPName']\n",
    "    mas_rows = mas[mas['MASName'] == dblp_name]\n",
    "    mas_matches = len(mas_rows)\n",
    "    \n",
    "    # Fullname: 1-to-N from DBLP to MAS \n",
    "    for index, mas_row in mas_rows.iterrows(): \n",
    "        if row['DBLPType'] != mas_row['MASType']: \n",
    "            print 'Distinct Type for: ', mas_row['MASName'], '!!!'\n",
    "            continue\n",
    "\n",
    "        row['MASID'] = mas_row['MASID']\n",
    "        row['MASName'] = mas_row['MASName']\n",
    "        row['MASShortName'] = mas_row['MASShortName']\n",
    "        row['MASSubDomainName'] = mas_row['MASSubDomainName']\n",
    "        \n",
    "        result = result.append(row)\n",
    "\n",
    "    if mas_matches > 0:\n",
    "        dblp_found[dblp_key] = 'True'\n",
    "        continue\n",
    "\n",
    "    # Second attempt: By Venue Shortname    \n",
    "    dblp_shortname = row['DBLPShortName']\n",
    "    mas_rows = mas[mas['MASShortName'] == dblp_shortname]\n",
    "    mas_matches = len(mas_rows)\n",
    "    \n",
    "    # Shortname: 1-to-1 from DBLP to MAS \n",
    "    if mas_matches == 1:\n",
    "        if row['DBLPType'] != mas_rows['MASType'].item(): \n",
    "            print 'Distinct Type for: ', mas_rows['MASName'].item(), '!!!'\n",
    "            continue\n",
    "            \n",
    "        row['MASName'] = mas_rows['MASName'].item()\n",
    "        row['MASID'] = mas_rows['MASID'].item()\n",
    "        row['MASShortName'] = mas_rows['MASShortName'].item()\n",
    "        row['MASSubDomainName'] = mas_rows['MASSubDomainName'].item()\n",
    "        \n",
    "        result = result.append(row)\n",
    "        dblp_found[dblp_key] = 'True'\n",
    "        continue\n",
    "\n",
    "    # Append empty row if didnt find\n",
    "    result = result.append(row)\n",
    "    \n",
    "# Sort\n",
    "result.sort('DBLPVenueKey', inplace=True)\n",
    "#result.drop_duplicates('DBLPVenueKey', inplace=True)\n",
    "result.reset_index(inplace=True)\n",
    "\n",
    "# Writing result files\n",
    "result = result.ix[:, ['DBLPVenueKey', 'MASID', 'MASSubDomainName', 'MASShortName', 'DBLPShortName', 'MASName', 'DBLPName']]\n",
    "result.to_csv('../conciliated_venues_FULL.csv')\n",
    "\n",
    "mini_result = result.ix[:, ['DBLPVenueKey', 'MASID', 'MASSubDomainName']]\n",
    "mini_result.to_csv('../conciliated_venues.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: DBLP Paper -> MAS SubArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "venues = pd.read_csv('../conciliated_venues.csv')\n",
    "paper_area = pd.DataFrame({}, [], columns=['Paper', 'Venue'])\n",
    "lines = []\n",
    "\n",
    "# Adding all CNPq author's publications\n",
    "with open('../dblp/cnpq-authors-dblpkeys.pubsfile') as dblp_file:\n",
    "    for line in dblp_file:\n",
    "        if '\\t' not in line: continue\n",
    "        lines.append(line[1:-1])    \n",
    "\n",
    "paper_area['Paper'] = lines\n",
    "paper_area['Venue'] = paper_area['Paper'].apply(\n",
    "    lambda x: \"\" if re.search(r'(.*/.*)/', x) is None else re.search(r'(.*/.*)/', x).group(1)\n",
    ")\n",
    "\n",
    "paper_area = pd.merge(paper_area, venues, left_on='Venue', right_on='DBLPVenueKey', how='left')\n",
    "paper_area.rename(columns={'MASSubDomainName':'Area'}, inplace=True)\n",
    "paper_area.sort('Area', inplace=True)\n",
    "paper_area.reset_index(inplace=True)\n",
    "\n",
    "paper_area = paper_area.ix[:, ['Paper', 'Area']]\n",
    "paper_area.to_csv('../paper-areas.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Scholar - CNPq Author's H-Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# H-Index File\n",
    "h = pd.read_csv('../cnpq/prod-levels.csv')\n",
    "h.rename(columns={'Name':'Author', 'H-index':'HIndex'}, inplace=True)\n",
    "h = h.ix[:, ['Author', 'HIndex']]\n",
    "h['HIndex'] = h[h['HIndex'].notnull()]['HIndex'].apply(lambda x: str(int(x)))\n",
    "h.to_csv('../author-hindex.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
