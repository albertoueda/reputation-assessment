{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['random', 'unwrap']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import hashlib\n",
    "import random\n",
    "import cookielib\n",
    "import sys\n",
    "import re\n",
    "import urllib\n",
    "from  urllib2 import *\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#from ghost import Ghost \n",
    "from lxml import html\n",
    "from unidecode import unidecode\n",
    "\n",
    "class Dict(dict):\n",
    "    def __missing__(self, key):\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliar Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sleep time to avoid hard hitting Google Scholar\n",
    "def sleep_and_build_opener():\n",
    "    time.sleep(MIN_SLEEP + (random.random() * MAX_RANDOM_MINUTES))  \n",
    "    global opener # TODO maybe \n",
    "    opener = build_opener(HTTPCookieProcessor()) \n",
    "    \n",
    "def open_allowed_page():\n",
    "    time.sleep(MIN_SLEEP)\n",
    "    opener.open('http://scholar.google.com.br/citations?user=JMkfK0sAAAAJ&hl=pt-PT') # Berthier's page\n",
    "    \n",
    "\n",
    "def open_page(url, action):\n",
    "    page = None\n",
    "    try:\n",
    "        sleep_and_build_opener()\n",
    "        page = opener.open(url) \n",
    "        open_allowed_page()\n",
    "    except URLError as e:     \n",
    "        print e\n",
    "        print \"Error in: \" + action\n",
    "        return None\n",
    "    except Exception as e:     \n",
    "        print e\n",
    "        print \"Generic Exception in: \" + action\n",
    "        return None\n",
    "    \n",
    "    if page != None and page.getcode() != 200:\n",
    "        print 'F**!!! Response: ' + str(page.getcode() + ' (' + action + ')')\n",
    "        return None\n",
    "    \n",
    "    return page\n",
    "\n",
    "def extract_element(page, xpath):\n",
    "    tree = html.fromstring(page.read())\n",
    "    return tree.xpath(xpath)\n",
    "\n",
    "\n",
    "def extract_author_info(nrc_file, index, link, author_page):            \n",
    "    tree = html.fromstring(author_page.read())\n",
    "\n",
    "    nrc_file.ix[index, 'GS_Name'] = tree.xpath(\"//div[@id='gsc_prf_in']/text()\")[0].encode('ascii', 'ignore')\n",
    "    nrc_file.ix[index, 'GS_H5'] = tree.xpath(\"(//td[@class='gsc_rsb_std'])[3]/text()\")[0]\n",
    "    nrc_file.ix[index, 'GS_Cits'] = tree.xpath(\"(//td[@class='gsc_rsb_std'])[1]/text()\")[0]\n",
    "    nrc_file.ix[index, 'GS_H5_2010'] = tree.xpath(\"(//td[@class='gsc_rsb_std'])[4]/text()\")[0]\n",
    "    nrc_file.ix[index, 'GS_Cits_2010'] = tree.xpath(\"(//td[@class='gsc_rsb_std'])[2]/text()\")[0]\n",
    "    nrc_file.ix[index, 'GS_Link'] = link\n",
    "\n",
    "### Not used\n",
    "def authenticate():\n",
    "    email = raw_input(\"Google username: \")\n",
    "    password = getpass.getpass(\"Password: \")\n",
    "\n",
    "    LOGIN_URL = 'https://accounts.google.com/ServiceLogin?service=grandcentral'\n",
    "    AUTH_URL = 'https://accounts.google.com/ServiceLoginAuth?service=grandcentral'\n",
    "    opener = build_opener(HTTPCookieProcessor()) \n",
    "\n",
    "    # Find GALX value\n",
    "    login_page_contents = opener.open(LOGIN_URL).read()\n",
    "    galx = re.search(r\"name=\\\"GALX\\\"\\s+type=\\\"hidden\\\"\\s+value=\\\"(.+)\\\"\", login_page_contents, re.IGNORECASE).group(1)\n",
    "    # print galx \n",
    "\n",
    "    login_data = urllib.urlencode({\n",
    "        'Email': email,\n",
    "        'Passwd': password,\n",
    "        'GALX': galx,\n",
    "    })\n",
    "\n",
    "    opener.open(AUTH_URL, login_data)\n",
    "\n",
    "def parse_nrc_data(nrc_file):\n",
    "    return nrc_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0 authors (0.0 seg)\n",
      "Progress: 10 authors (125.7 seg)\n",
      "Progress: 20 authors (252.8 seg)\n",
      "Progress: 30 authors (425.8 seg)\n",
      "Progress: 40 authors (577.3 seg)\n",
      "Progress: 50 authors (1974.8 seg)\n",
      "Progress: 60 authors (2100.2 seg)\n",
      "Progress: 70 authors (2228.8 seg)\n",
      "Progress: 80 authors (2354.3 seg)\n",
      "Progress: 90 authors (2476.3 seg)\n",
      "Progress: 100 authors (2600.1 seg)\n",
      "Progress: 110 authors (2730.9 seg)\n",
      "Progress: 120 authors (2862.3 seg)\n",
      "Progress: 130 authors (2991.7 seg)\n",
      "Progress: 140 authors (3104.3 seg)\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode character u'\\xf3' in position 12: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-215-45d9a289c038>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# Fullname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mfull_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munidecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mENCODING\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# Author already found\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/alberto/anaconda/lib/python2.7/encodings/utf_8.pyc\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(input, errors)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutf_8_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character u'\\xf3' in position 12: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "N_AUTHORS = 1000\n",
    "EXCLUDED_LASTNAMES = [\"FILHO\", \"NETO\", \"JUNIOR\", \"JR\"]\n",
    "MIN_SLEEP = 2\n",
    "MAX_RANDOM_MINUTES = 0\n",
    "AUTHOR_URL_XPATH = \"(//h3[@class='gsc_1usr_name']/a)[1]/@href\"\n",
    "ENCODING = 'utf-8'\n",
    "\n",
    "opener = build_opener(HTTPCookieProcessor()) \n",
    "found_fullnames = Dict()\n",
    "\n",
    "# NRC input file\n",
    "nrc_file = pd.read_json(\"../nrc/cs-authors-nrc.json\")\n",
    "parse_nrc_data(nrc_file)\n",
    "\n",
    "k = 0\n",
    "h_index_not_found = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for (index, row) in nrc_file.iterrows():\n",
    "    if k % 10 == 0: print (\"Progress: \" + str(k) + \" authors (%.1f seg)\" % (time.time() - start_time))    \n",
    "    k += 1 \n",
    "    \n",
    "    if (k > N_AUTHORS): break\n",
    "\n",
    "    # Fullname\n",
    "    full_name = unidecode(row['name'].decode(ENCODING)).upper()\n",
    "    \n",
    "    # Author already found\n",
    "    if found_fullnames[full_name]: \n",
    "        print 'A-ha I will not search again for: ' + full_name\n",
    "        continue\n",
    "    \n",
    "    names = full_name.split()\n",
    "    lastname = names[len(names)-1]        \n",
    "    if (lastname.upper() in EXCLUDED_LASTNAMES and len(names) > 2): lastname = names[len(names)-2]\n",
    "    short_name = names[0] + ' ' + lastname\n",
    "    # print \"\\n\" + short_name\n",
    "\n",
    "    # Specific Known Cases\n",
    "    if full_name == 'Wagner Meira Junior':\n",
    "        full_name = short_name = 'Wagner Meira'\n",
    "    elif full_name == 'EDMUNDO ALBUQUERQUE DE SOUZA E SILVA':\n",
    "        full_name = short_name = 'EDMUNDO SOUZA SILVA'\n",
    "\n",
    "    # University\n",
    "    university = unidecode(row['group_name'].decode(ENCODING))\n",
    "\n",
    "    # 1st try: Fullname search\n",
    "    fullname_search_url = 'http://scholar.google.com.br/citations?view_op=search_authors&mauthors=' + urllib.quote(full_name)\n",
    "    search_page = open_page(fullname_search_url, 'Fullname searching')\n",
    "    if search_page is None: break\n",
    "\n",
    "    author_id = extract_element(search_page, AUTHOR_URL_XPATH)\n",
    "\n",
    "    # 2nd try: Shortname + University search\n",
    "    if not author_id: \n",
    "        shortname_search_url = 'http://scholar.google.com.br/citations?view_op=search_authors&mauthors=' + urllib.quote(short_name + ' ' + university)\n",
    "        search_page = open_page(shortname_search_url, 'Shortname + University searching')\n",
    "        if search_page is None: break\n",
    "\n",
    "        author_id = extract_element(search_page, AUTHOR_URL_XPATH)\n",
    "\n",
    "    # Giving up, jump to next author        \n",
    "    if not author_id: \n",
    "        h_index_not_found += 1\n",
    "    else:\n",
    "        # Found author!\n",
    "        author_id = author_id[0]\n",
    "        author_url = \"https://scholar.google.com.br\" + author_id\n",
    "        author_page = open_page(author_url, 'Opening author profile')\n",
    "        if author_page is None: break\n",
    "\n",
    "        found_fullnames[full_name] = True\n",
    "        extract_author_info(nrc_file, index, author_id, author_page)\n",
    "\n",
    "# End for \n",
    "nrc_file.to_csv('../h-index/nrc-h-index-bot-output.csv', encoding='utf-8')    \n",
    "\n",
    "print(\"Authors Not Found in GSC: %d of %d (%.1f%%)\" % (h_index_not_found, k-1, (h_index_not_found * 100.00) / (k-1)))\n",
    "print(\"\\n--- Finished after %.3f seconds. ---\" % (time.time() - start_time))\n",
    "\n",
    "del opener\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NRC Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group_NRC</th>\n",
       "      <th>group_capesMaxDMF</th>\n",
       "      <th>group_id</th>\n",
       "      <th>group_name</th>\n",
       "      <th>group_publication_count</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1743</th>\n",
       "      <td> [17, 39]</td>\n",
       "      <td> 0</td>\n",
       "      <td> 53e3e85631522e24717ad81b</td>\n",
       "      <td> Louisiana State University and Agricultural an...</td>\n",
       "      <td> 1142</td>\n",
       "      <td> 53e4e8e962c22434d8beedae</td>\n",
       "      <td> Gerald Baumgartner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6950</th>\n",
       "      <td>     None</td>\n",
       "      <td> 0</td>\n",
       "      <td> 553cf2cd62c2242e37c7ae0f</td>\n",
       "      <td>                &lt;&lt;&lt; artificial reference group &gt;&gt;&gt;</td>\n",
       "      <td>    0</td>\n",
       "      <td> 53e4e8e962c22434d8beedae</td>\n",
       "      <td> Gerald Baumgartner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     group_NRC  group_capesMaxDMF                  group_id  \\\n",
       "1743  [17, 39]                  0  53e3e85631522e24717ad81b   \n",
       "6950      None                  0  553cf2cd62c2242e37c7ae0f   \n",
       "\n",
       "                                             group_name  \\\n",
       "1743  Louisiana State University and Agricultural an...   \n",
       "6950                 <<< artificial reference group >>>   \n",
       "\n",
       "      group_publication_count                        id                name  \n",
       "1743                     1142  53e4e8e962c22434d8beedae  Gerald Baumgartner  \n",
       "6950                        0  53e4e8e962c22434d8beedae  Gerald Baumgartner  "
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrc_file = pd.read_json(\"../nrc/cs-authors-nrc.json\")\n",
    "nrc_groupby_name = nrc_file.groupby('name')['group_name'].size()\n",
    "names = nrc_groupby_name[nrc_groupby_name > 2] # without .size() again!\n",
    "# print names\n",
    "#nrc_file.ix[nrc_file['group_name']=='Louisiana State University and Agricultural and Mechanical College', 'name':]#['name', 'group_name', 'group_id', 'group_NRC', 'group_publication_count']]\n",
    "nrc_file.ix[nrc_file['name']=='Gerald Baumgartner', :]\n",
    "# nrc_file[nrc_file['group_name'] == '<<< artificial reference group >>>']\n",
    "\n",
    "# %timeit x = 'a'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
